---
title: "Stat 585 - Web Scraping"
author: "Heike Hofmann and Susan Vanderplas"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, default-fonts, "tweaks.css"]
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup, include=FALSE, message=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi= 300)
options(width=60)
library(tidyverse)
```



## Web Scraping 

- Transform data from web pages into usable information

- Automate the process

![](http://webdata-scraping.com/media/2015/01/Web-Scraping-Process.png)

---

## `rvest` + `xml2`: Easy Web Scraping
  
- `read_html` gets the full set of HTML markup from a URL

```{r warning = FALSE, message = FALSE}
library(rvest)
url <- "https://www.nytimes.com/elections/2016/results/iowa"
html <- read_html(url)
html
```

- Use `html_attr`, `html_node`, `html_table`, and `html_text` to extract useful information from the markup

---

## Get a *table* from an online source

`html_table` extracts all tables from the sourced html into a list of data frames:
  
```{r}
tables <- html %>% html_table(fill=TRUE)
tables %>% purrr::map(glimpse)
```

---

## Data Munging

Most tables need a bit of clean-up:

```{r}
ia_results <- tables[[2]] %>% mutate(
  Trump  = parse_number(Trump),
  Clinton = parse_number(Clinton)
)
ia_results
```
---
class:inverse
## Your Turn 

Connect to the website of the NY Times election results at https://www.nytimes.com/elections/2016/results/president
  
- Pick your favorite state (by clicking on the state on the map)

- Use `rvest` to download the election results for the 2016 Presidential Election for all counties of your state.

- Clean up the data (numbers should be numbers).

- Convert the numbers into row-wise percentages (decide on either %Republican or %Democrat).


---
class:inverse
## Your Turn 

How well does the web-scraping work over time? 
Results for the 2020 Election are available from https://www.nytimes.com/interactive/2020/11/03/us/elections/results-president.html

Again, pick your favorite state and apply your previous code. 

How many things have changed? Which format do you prefer? Why?

---

## Your Turn - Solutions

```{r}
ia2020 <- "https://www.nytimes.com/interactive/2020/11/03/us/elections/results-iowa.html"
html <- read_html(ia2020)
tables <- html %>% html_table(fill=TRUE)
tables %>% purrr::map(glimpse)
ia_results_2020 <- tables[[2]][,-4] %>% mutate(
  `Total votes` = parse_number(`Total votes`),
  Absentee = parse_number(Absentee),
  Winner = gsub("(.*) +.*", "\\1", Margin),
  `Margin 2020` = gsub("(.*) (+.*)", "\\2", Margin),
  `Margin 2020` = ifelse(Winner=="Biden", paste0("D",`Margin 2020`), paste0("R",`Margin 2020`))
)
ia_results_2020
```


---
class:inverse
## Your Turn 

Can you get the data you just scraped onto a map of the counties? - Try to map the percentage you calculated to the fill-color of the counties' polygons.

You will need to `join` the percentages with the county polygons:

```{r message=FALSE}
counties <- map_data("county")
head(counties)
```

---

## Your Turn - Solution

```{r}
ia_results <- ia_results %>% mutate(
  PercDem = Clinton/(Trump + Clinton)*100,
  subregion = tolower(`Vote by county`)
) 

# Which counties don't work?
anti_join(ia_results, counties %>% filter(region=="iowa"), by="subregion")
anti_join(counties %>% filter(region=="iowa"), ia_results,  by="subregion")
```

---

## Your Turn - Solution (cont'd)

```{r}
ia_results <- ia_results %>% mutate(
  subregion = replace(subregion, subregion=="o'brien", "obrien")
) 

ia_full <- left_join(ia_results, 
                     counties %>% filter(region=="iowa"), 
                     by="subregion")
```

---

## Your Turn - Solution (cont'd)

```{r, fig.width = 6, fig.height = 4, out.width = "75%"}
ia_full %>% 
  ggplot(aes( x = long, y = lat, group = subregion)) +
  geom_polygon(aes(fill = PercDem), color = "black") + 
  scale_fill_gradient2(midpoint=50, mid = "white") +
  ggthemes::theme_map()
```

---

## Beyond tables

Sometimes data on the web is not structured as nicely... e.g. let's assume we want to get a list of all recently active baseball players from [Baseball reference](http://www.baseball-reference.com/players/)

.center[![:scale 80%](baseball_reference.png)]

---

## SelectorGadget

- SelectorGadget is a javascript bookmarklet to determine the css selectors of pieces of a website we want to extract.

- Bookmark the [SelectorGadget](https://selectorgadget.com/) link, then click on it to use it (or add the chrome extension)

- When SelectorGadget is active, pieces of the website are highlighted in orange/green/red.

- Use SelectorGadget on http://www.baseball-reference.com/players/ .

- Read more details on `vignette("selectorgadget")` (or on the [rvest website](https://rvest.tidyverse.org/articles/selectorgadget.html))

If you prefer, you can also read the HTML code and create your own [CSS](https://www.w3schools.com/cssref/css_selectors.asp) or [XPATH](https://www.w3schools.com/xml/xpath_syntax.asp) selectors. 

---

## SelectorGadget Result

*Select all active baseball players with a last name starting with 'a'*

```{r}
url <- "http://www.baseball-reference.com/players/a/"
html <- read_html(url)
html %>% html_elements("b") %>% html_text()
```

---

## Example, varied

We are, in fact, not just interested in the *names of the players*, but also in the *links* to each player's website

- `html_attr` let's us access an attribute of an html node

- `html_attrs` extracts all attributes of an html node

```{r}
html %>% html_elements("b a") %>% html_attr(name="href")
```

---
class:inverse
## Your Turn

Use the SelectorGadget on the website for [Fernando Abad](https://www.baseball-reference.com/players/a/abadfe01.shtml)

Find the css selector to extract his career statistics and load them into your R session.

Does the same code work to extract career statistics for (some)  of the other active players? 

What other information do we need to know? - and how can we get to that?

---

## Your Turn  - Solution

```{r}
url <- "https://www.baseball-reference.com/players/a/abadfe01.shtml"
html <- read_html(url)
html %>% html_elements(".stats_pullout") %>% html_text()
html %>% html_elements(".stats_pullout .poptip") %>% html_text()
html %>% html_elements(".p3 p , .p2 p, .p1 p, .stats_pullout strong") %>% html_text()

html %>% html_elements(".p1") %>% html_text()
```
---

## Your Turn - Solution (cont'd)

It's sometimes easier (for data munging after extracting) to extract multiple pieces rather than everything in one go. 
```{r}
(stats <- html %>% html_elements("span strong") %>% html_text())
(season <- html %>% html_elements(".stats_pullout p:nth-child(2)") %>% html_text())
(career <- html %>% html_elements(".stats_pullout p:nth-child(3)") %>% html_text())
```

---
class:inverse
## Your Turn

Clean up the code for extracting the career statistics and write a function `getStats` that takes a url and returns a dataset (or tibble).

Does the same code work to extract career statistics for (some)  of the other active players? 

---

## Your Turn  - Solution 

```{r}
getStats <- function(url) {
  html <- read_html(url)
  stats <- html %>% html_nodes("span strong") %>% html_text()
  season <- html %>% html_nodes(".stats_pullout p:nth-child(2)") %>% html_text()
  career <- html %>% html_nodes(".stats_pullout p:nth-child(3)") %>% html_text()
  
  data.frame(Statistics = stats[-1], 
             Season= parse_number(season[-1]),
             Career = ifelse(is.null(career), NA, parse_number(career[-1])))
}

url <- "http://www.baseball-reference.com/players/a/"
html <- read_html(url)
players <- html %>% html_nodes("strong a") %>% html_text()
links <- html %>% html_nodes("strong a") %>% html_attr(name="href")


getStats("https://www.baseball-reference.com//players/a/abreujo02.shtml") %>% str()
getStats(file.path("https://www.baseball-reference.com", links[2])) %>% str()
# getStats(file.path("https://www.baseball-reference.com", links[3])) %>% str() # gives an error
getStats(file.path("https://www.baseball-reference.com", links[4])) %>% str()
```


---

## Your Turn  - Solution (cont'd)

Now apply to other players (with 'a' as starting letter) - first we get everything tidied up in a dataset

```{r}
url <- "http://www.baseball-reference.com/players/a/"
html <- read_html(url)
players <- html %>% html_nodes("strong a") %>% html_text()
links <- html %>% html_nodes("strong a") %>% html_attr(name="href")

bb <- tibble(players=players, links = links)
head(bb)

bb_head <- bb[1:5,] %>% mutate(
  data = links %>% purrr::map(.f = function(link) {
    cat(link)
    cat("\n")
    getStats(file.path("https://www.baseball-reference.com", link))
  })
)
```

---
class: inverse
## Your Turn

Why does 
`getStats(file.path("https://www.baseball-reference.com", "/players/a/adlemti01.shtml"))`
have NAs in the Career?

Is this fixable?

How do we get all of the active players? (not just the ones with last names starting with 'a') 

---

## Package `rvest`


The `session` suite of commands allows to simulate an html session without a browser.

Create a session with `session(url)`

Navigate: `session_jump_to()` 
Follow a link: `session_follow_link()`.

navigate back and forward with `session_back()` and `session_forward()`.

... and extract the pieces you are interested in using `read_html`, `html_element`, `html_elements`
